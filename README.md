# ğŸ“Œ Web Crawling e AnÃ¡lise de Texto

Projeto de **web crawling** e **anÃ¡lise de textos** com **persistÃªncia em banco de dados MySQL**.
O objetivo Ã© coletar artigos da web, extrair o conteÃºdo em granularidade de frases, calcular mÃ©tricas linguÃ­sticas e sentimentais e armazenar em banco de forma incremental (sem duplicaÃ§Ãµes).

---

## ğŸ› ï¸ Tecnologias utilizadas

* **Linguagem**: Python
* **Banco de dados**: MySQL
* **Sistema Operacional**: Realizado no Windows,

### Bibliotecas

* [BeautifulSoup4] â†’ Parsing HTML e extraÃ§Ã£o de conteÃºdo
* [NLTK] â†’ TokenizaÃ§Ã£o, anÃ¡lise de frases e palavras
* [textstat]â†’ MÃ©tricas de legibilidade
* [TextBlob] â†’ AnÃ¡lise de sentimento e subjetividade
* [mysql-connector-python]â†’ ConexÃ£o com MySQL

---

## ğŸ“‚ Estrutura do Projeto

```
web-crawling/
â”‚
â”œâ”€â”€ handler.py         # Orquestra o fluxo: scraping, cÃ¡lculo de mÃ©tricas e inserÃ§Ã£o no banco
â”œâ”€â”€ connection.py      # ConexÃ£o com MySQL, criaÃ§Ã£o da tabela, inserÃ§Ã£o incremental e leitura
â”œâ”€â”€ README.md          # DocumentaÃ§Ã£o do projeto
â””â”€â”€ requirements.txt   # DependÃªncias do projeto
```

### ğŸ”¹ **handler.py**

* `calcular_metricas(texto)` â†’ Calcula todas as mÃ©tricas linguÃ­sticas e sentimentais.
* `get_content(url, tag)` â†’ Faz scraping da URL, extrai parÃ¡grafos e insere no banco de forma incremental.

### ğŸ”¹ **connection.py**

* `conectar_mysql()` â†’ Cria conexÃ£o com MySQL.
* `fechar_conexao(conexao)` â†’ Fecha a conexÃ£o.
* `criar_tabela()` â†’ Cria tabela `content_tech` se nÃ£o existir.
* `inserir_content_tech(dados)` â†’ Insere dados incrementalmente (verificando `url` + `frases`).
* `ler_todos_content_tech()` â†’ LÃª todos os registros da tabela.

---

## âš™ï¸ InstalaÃ§Ã£o

### Clone o repositÃ³rio

```bash
git clone https://github.com/OlavoFerrazNeto/web-crawling.git
cd web-crawling
```

### Crie um ambiente virtual

Linux/MacOS:

```bash
python3 -m venv venv
source venv/bin/activate
```

Windows:

```bash
python -m venv venv
venv\Scripts\activate
```

### Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

ConteÃºdo de `requirements.txt`:

```
bs4
nltk
textstat
textblob
mysql-connector-python
```

### Configure o banco MySQL

CriaÃ§Ã£o do banco mysql:

```sql
CREATE DATABASE database_on_case;
```

Configure o usuÃ¡rio e senha em `connection.py`.

CriaÃ§Ã£o da tabela content_tech:
ExecuÃ§Ã£o da funÃ§Ã£o criar_tabela do arquivo connection.py

### ExecuÃ§Ã£o do projeto

```bash
python handler.py
```

---

## DecisÃµes tomadas

1. **Coleta de dados**: Optei pelo **BeautifulSoup** por ser simples e eficaz em parsing HTML.
2. **Processamento NLP**: Utilizei **NLTK** para tokenizaÃ§Ã£o e contagem de palavras/frases.
3. **MÃ©tricas de legibilidade**: Adicionei **textstat** por ser especializado nesses cÃ¡lculos.
4. **Sentimento e subjetividade**: Escolhi **TextBlob** por oferecer API simples para anÃ¡lise emocional.
5. **PersistÃªncia incremental**: Usei `mysql-connector-python` para garantir que `url + frases` sejam Ãºnicos e evitar duplicaÃ§Ãµes.
6. **Granularidade**: Decidi salvar os artigos em **frases**, pois facilita anÃ¡lises detalhadas.

---

## MÃ©tricas de Performance

* **Tempo mÃ©dio de processamento por artigo**: \~2-4 segundos (dependendo do tamanho do texto).
* **Carga incremental**: Evita duplicaÃ§Ãµes e mantÃ©m o banco sempre atualizado.
* **Escalabilidade**:

  * Em cenÃ¡rios maiores, recomenda-se:

    * Uso de **fila (RabbitMQ, Kafka)** para distribuir scraping.
    * **Paralelismo** com `asyncio` ou `multiprocessing`. Com o objetivo de executar vÃ¡rias tarefas ao mesmo tempo.
    * **Armazenamento distribuÃ­do** (PostgreSQL, BigQuery ou Data Lake).
    * **Agendador** (Cron) para rodar em um horÃ¡rio programado.

---

## ConclusÃ£o geral

Esse projeto permite:
- Coleta automatizada de artigos.
-Processamento linguÃ­stico e sentimental.
-Armazenamento incremental em banco MySQL.
-Pronto para consultas analÃ­ticas e escalÃ¡vel para cenÃ¡rios maiores.


